import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import json


def load_config():
    try:
        with open("config.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print("[*] config.json bulunamadƒ±!")
        return None


def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_k=50, top_p=0.9):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_length=max_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            num_return_sequences=1
        )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text


def main():
    print("=" * 60)
    print("[*] VortexAI - GPT-2 Metin √úretici")
    print("=" * 60)
    
    config = load_config()
    if not config:
        return
    
    model_name = config["files"]["model_name"]
    
    print(f"\nüì• Model y√ºkleniyor: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()
        
        print(f"[*] Model y√ºklendi")
        print(f"[*] Cihaz: {device}")
    except Exception as e:
        print(f"[*] Model y√ºklenemedi: {e}")
        print("[*] √ñnce eƒüitim yapƒ±n: python train_gpt2.py")
        return
    
    print("\n" + "=" * 60)
    print("Komutlar:")
    print("  - Metin girin ‚Üí Devamƒ±nƒ± √ºretir")
    print("  - 'temp X' ‚Üí Sƒ±caklƒ±k (0.5=g√ºvenli, 1.5=yaratƒ±cƒ±)")
    print("  - 'len X' ‚Üí Uzunluk ayarƒ±")
    print("  - 'quit' ‚Üí √áƒ±kƒ±≈ü")
    print("=" * 60 + "\n")
    
    temperature = 1.0
    max_length = 50
    
    while True:
        try:
            user_input = input("[*] Prompt: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() == "quit":
                print("[*] G√∂r√º≈ümek √ºzere!")
                break
            
            if user_input.lower().startswith("temp "):
                try:
                    temperature = float(user_input.split()[1])
                    print(f"[*]üå°Ô∏è  Sƒ±caklƒ±k: {temperature}")
                except:
                    print("[*] Ge√ßersiz! √ñrnek: temp 1.5")
                continue
            
            if user_input.lower().startswith("len "):
                try:
                    max_length = int(user_input.split()[1])
                    print(f"[*] Uzunluk: {max_length}")
                except:
                    print("[*] Ge√ßersiz! √ñrnek: len 100")
                continue
            
            print("\nü§ñ VortexAI d√º≈ü√ºn√ºyor...")
            result = generate_text(
                model,
                tokenizer,
                user_input,
                max_length=max_length,
                temperature=temperature
            )
            
            print(f"\n[*] VortexAI:\n{result}\n")
            print("-" * 60 + "\n")
        
        except KeyboardInterrupt:
            print("\n[*] G√∂r√º≈ümek √ºzere!")
            break
        except Exception as e:
            print(f"[*] Hata: {e}")


if __name__ == "__main__":
    main()